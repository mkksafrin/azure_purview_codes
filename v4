"""
Azure Purview Data Extraction Script - Two-Step Bulk API Approach
==================================================================

This script extracts metadata from Azure Purview using a two-step process:

STEP 1: Fetch Table Entities
    - Search for entities by type (azure_datalake_gen2_resource_set, azure_synapse_dedicated_sql_table)
    - Get entity IDs from search results
    - Call bulk API with entity IDs to fetch table metadata

STEP 2: Extract Schema GUIDs
    - From the table entity response, extract GUIDs from:
      entities -> relationshipAttributes -> attachedSchema -> guid
    - These GUIDs represent column/schema entities

STEP 3: Fetch Column Entities
    - Call bulk API again with schema GUIDs
    - Fetch column-level metadata (name, data type, description)

STEP 4: Combine and Extract
    - Merge table metadata with column metadata
    - Create DataFrame with 13 required fields
    - Write to Azure Data Lake Storage

Output Fields:
1. table_asset_id - Table GUID from entities.guid
2. column_guid - Column GUID from schema entities.guid
3. collectionId - From entities.collectionId
4. entity_type - From entities.relationshipAttributes.relatedDataSets.typeName
5. col_name - From schema entities.attributes.name
6. data_type - From schema entities.attributes.type
7. desc - From schema entities.attributes.userDescription
8. qName - From schema entities.attributes.qualifiedName
9. classification - From entities.classifications.typeName
10. createTime - From entities.createTime
11. updateTime - From entities.updateTime
12. entityStatus - From entities.status
13. lastModifiedTS - From entities.lastModifiedTS

Features:
- Handles pagination in search results
- Processes entities in batches (100 per batch)
- Processes schemas in sub-batches (100 per batch)
- Rate limiting to avoid API throttling
- Comprehensive error handling
- Uploads final CSV to Azure Data Lake Storage
"""

import requests
import pandas as pd
from msal import ConfidentialClientApplication
import json
from typing import List, Dict, Any
import time
from azure.storage.filedatalake import DataLakeServiceClient
from datetime import datetime

# Configuration
ACCOUNT_NAME = "your-purview-account"
TENANT_ID = "your-tenant-id"
CLIENT_ID = "your-client-id"
CLIENT_SECRET = "your-client-secret"

# ADLS Configuration
ADLS_ACCOUNT_NAME = "your-adls-account"
ADLS_FILE_SYSTEM = "your-container"
ADLS_PATH = "purview-extract/data.csv"

# Entity types to search for
ENTITY_TYPES = [
    "azure_datalake_gen2_resource_set",
    "azure_synapse_dedicated_sql_table"
]


class AzurePurviewExtractor:
    """Extract metadata from Azure Purview API"""
    
    def __init__(self, account_name: str, tenant_id: str, client_id: str, client_secret: str):
        self.account_name = account_name
        self.tenant_id = tenant_id
        self.client_id = client_id
        self.client_secret = client_secret
        self.access_token = None
        self.base_url = f"https://{account_name}.purview.azure.com"
        
    def get_access_token(self) -> str:
        """Authenticate using MSAL and get access token"""
        authority = f"https://login.microsoftonline.com/{self.tenant_id}"
        scope = ["https://purview.azure.net/.default"]
        
        app = ConfidentialClientApplication(
            client_id=self.client_id,
            client_credential=self.client_secret,
            authority=authority
        )
        
        result = app.acquire_token_for_client(scopes=scope)
        
        if "access_token" in result:
            self.access_token = result["access_token"]
            print("Successfully authenticated with Azure Purview")
            return self.access_token
        else:
            raise Exception(f"Authentication failed: {result.get('error_description', 'Unknown error')}")
    
    def search_entities(self, entity_type: str, limit: int = 1000) -> List[str]:
        """
        Search for entities by type and return their IDs with pagination
        """
        url = f"{self.base_url}/catalog/api/search/query?api-version=2022-08-01-preview"
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Content-Type": "application/json"
        }
        
        all_entity_ids = []
        offset = 0
        
        while True:
            payload = {
                "keywords": "*",
                "filter": {
                    "entityType": entity_type
                },
                "limit": limit,
                "offset": offset
            }
            
            print(f"Searching for {entity_type} entities (offset: {offset})...")
            response = requests.post(url, headers=headers, json=payload)
            
            if response.status_code != 200:
                print(f"Search failed: {response.status_code} - {response.text}")
                break
            
            data = response.json()
            results = data.get("value", [])
            
            if not results:
                print(f"No more results for {entity_type}")
                break
            
            # Extract IDs from search results
            entity_ids = [entity.get("id") for entity in results if entity.get("id")]
            all_entity_ids.extend(entity_ids)
            
            print(f"Found {len(entity_ids)} entities in this batch (total: {len(all_entity_ids)})")
            
            # Check if there are more results
            total_count = data.get("@search.count", 0)
            if offset + limit >= total_count:
                break
            
            offset += limit
            time.sleep(0.5)  # Rate limiting
        
        return all_entity_ids
    
    def get_entity_bulk(self, entity_ids: List[str]) -> Dict[str, Any]:
        """
        Fetch entity details in bulk using the bulk API
        """
        url = f"{self.base_url}/catalog/api/atlas/v2/entity/bulk"
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Content-Type": "application/json"
        }
        
        params = {
            "guid": entity_ids,
            "ignoreRelationships": False,
            "minExtInfo": False
        }
        
        response = requests.get(url, headers=headers, params=params)
        
        if response.status_code != 200:
            raise Exception(f"Bulk API failed: {response.status_code} - {response.text}")
        
        return response.json()
    
    def get_schema_bulk(self, schema_guids: List[str]) -> Dict[str, Any]:
        """
        Fetch schema/column details in bulk using the bulk API
        """
        url = f"{self.base_url}/catalog/api/atlas/v2/entity/bulk"
        headers = {
            "Authorization": f"Bearer {self.access_token}",
            "Content-Type": "application/json"
        }
        
        params = {
            "guid": schema_guids,
            "ignoreRelationships": False,
            "minExtInfo": False
        }
        
        response = requests.get(url, headers=headers, params=params)
        
        if response.status_code != 200:
            raise Exception(f"Schema Bulk API failed: {response.status_code} - {response.text}")
        
        return response.json()
    
    def extract_schema_guids(self, entity_bulk_response: Dict[str, Any]) -> List[str]:
        """
        Extract GUIDs from entities->relationshipAttributes->attachedSchema
        """
        schema_guids = []
        entities = entity_bulk_response.get("entities", [])
        
        for entity in entities:
            relationship_attrs = entity.get("relationshipAttributes", {})
            attached_schema = relationship_attrs.get("attachedSchema", [])
            
            # Handle both list and single object
            if isinstance(attached_schema, list):
                for schema in attached_schema:
                    if isinstance(schema, dict) and "guid" in schema:
                        schema_guids.append(schema["guid"])
            elif isinstance(attached_schema, dict) and "guid" in attached_schema:
                schema_guids.append(attached_schema["guid"])
        
        return schema_guids
    
    def extract_metadata(self, entity_bulk_response: Dict[str, Any], 
                         schema_bulk_response: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Extract required fields from entity and schema bulk API responses
        
        Args:
            entity_bulk_response: Response from bulk API with table entities
            schema_bulk_response: Response from bulk API with schema/column entities
        """
        extracted_data = []
        
        # First, create a mapping of table GUIDs to their metadata
        table_metadata = {}
        entities = entity_bulk_response.get("entities", [])
        
        for entity in entities:
            table_guid = entity.get("guid")
            collection_id = entity.get("collectionId")
            create_time = entity.get("createTime")
            update_time = entity.get("updateTime")
            entity_status = entity.get("status")
            last_modified_ts = entity.get("lastModifiedTS")
            
            # Extract classifications
            classifications = entity.get("classifications", [])
            classification_names = [c.get("typeName") for c in classifications if c.get("typeName")]
            classification_str = ", ".join(classification_names) if classification_names else None
            
            # Extract related datasets
            relationship_attrs = entity.get("relationshipAttributes", {})
            related_datasets = relationship_attrs.get("relatedDataSets", [])
            
            # Handle both list and single object
            if not isinstance(related_datasets, list):
                related_datasets = [related_datasets] if related_datasets else []
            
            dataset_entity_types = []
            for dataset in related_datasets:
                if isinstance(dataset, dict) and "typeName" in dataset:
                    dataset_entity_types.append(dataset["typeName"])
            
            entity_type_str = ", ".join(dataset_entity_types) if dataset_entity_types else None
            
            # Store table metadata
            table_metadata[table_guid] = {
                "table_asset_id": table_guid,
                "collectionId": collection_id,
                "entity_type": entity_type_str,
                "classification": classification_str,
                "createTime": create_time,
                "updateTime": update_time,
                "entityStatus": entity_status,
                "lastModifiedTS": last_modified_ts
            }
        
        # Now process schema/column entities
        schema_entities = schema_bulk_response.get("entities", [])
        referred_entities = schema_bulk_response.get("referredEntities", {})
        
        for schema_entity in schema_entities:
            column_guid = schema_entity.get("guid")
            attributes = schema_entity.get("attributes", {})
            
            # Get column details
            col_name = attributes.get("name")
            data_type = attributes.get("type") or attributes.get("data_type")
            user_description = attributes.get("userDescription")
            q_name = attributes.get("qualifiedName")
            
            # Find parent table from relationshipAttributes
            relationship_attrs = schema_entity.get("relationshipAttributes", {})
            
            # Try different possible parent relationships
            parent_table = None
            parent_guid = None
            
            # Check various relationship attributes for parent table
            for rel_key in ["table", "parentTable", "entity", "__Table"]:
                if rel_key in relationship_attrs:
                    parent_ref = relationship_attrs[rel_key]
                    if isinstance(parent_ref, dict) and "guid" in parent_ref:
                        parent_guid = parent_ref["guid"]
                        break
            
            # If parent GUID found in table_metadata, use it; otherwise skip or use referred entities
            if parent_guid and parent_guid in table_metadata:
                table_info = table_metadata[parent_guid]
            else:
                # Try to find parent in referred entities or use empty values
                table_info = {
                    "table_asset_id": parent_guid,
                    "collectionId": None,
                    "entity_type": None,
                    "classification": None,
                    "createTime": None,
                    "updateTime": None,
                    "entityStatus": None,
                    "lastModifiedTS": None
                }
            
            # Create record combining table and column info
            record = {
                "table_asset_id": table_info["table_asset_id"],
                "column_guid": column_guid,
                "collectionId": table_info["collectionId"],
                "entity_type": table_info["entity_type"],
                "col_name": col_name,
                "data_type": data_type,
                "desc": user_description,
                "qName": q_name,
                "classification": table_info["classification"],
                "createTime": table_info["createTime"],
                "updateTime": table_info["updateTime"],
                "entityStatus": table_info["entityStatus"],
                "lastModifiedTS": table_info["lastModifiedTS"]
            }
            
            extracted_data.append(record)
        
        # If no schema entities were found, create records for tables without columns
        if not schema_entities:
            for table_guid, table_info in table_metadata.items():
                record = {
                    "table_asset_id": table_info["table_asset_id"],
                    "column_guid": None,
                    "collectionId": table_info["collectionId"],
                    "entity_type": table_info["entity_type"],
                    "col_name": None,
                    "data_type": None,
                    "desc": None,
                    "qName": None,
                    "classification": table_info["classification"],
                    "createTime": table_info["createTime"],
                    "updateTime": table_info["updateTime"],
                    "entityStatus": table_info["entityStatus"],
                    "lastModifiedTS": table_info["lastModifiedTS"]
                }
                extracted_data.append(record)
        
        return extracted_data
    
    def process_in_batches(self, entity_ids: List[str], batch_size: int = 100) -> pd.DataFrame:
        """
        Process entity IDs in batches with two-step bulk API approach:
        1. Fetch entity details (tables)
        2. Extract schema GUIDs from attachedSchema
        3. Fetch schema details (columns)
        4. Extract and combine metadata
        """
        all_data = []
        total_batches = (len(entity_ids) + batch_size - 1) // batch_size
        
        for i in range(0, len(entity_ids), batch_size):
            batch = entity_ids[i:i + batch_size]
            batch_num = (i // batch_size) + 1
            
            print(f"Processing batch {batch_num}/{total_batches} ({len(batch)} entities)...")
            
            try:
                # Step 1: Get entity details (tables)
                print(f"  Step 1: Fetching entity details...")
                entity_bulk_response = self.get_entity_bulk(batch)
                
                # Step 2: Extract schema GUIDs from entities->relationshipAttributes->attachedSchema
                print(f"  Step 2: Extracting schema GUIDs from attachedSchema...")
                schema_guids = self.extract_schema_guids(entity_bulk_response)
                
                if schema_guids:
                    print(f"  Found {len(schema_guids)} schema GUIDs")
                    
                    # Step 3: Fetch schema details in sub-batches if needed
                    schema_batch_size = 100
                    schema_bulk_response = {"entities": [], "referredEntities": {}}
                    
                    for j in range(0, len(schema_guids), schema_batch_size):
                        schema_batch = schema_guids[j:j + schema_batch_size]
                        print(f"  Step 3: Fetching schema details ({j+1}-{min(j+schema_batch_size, len(schema_guids))} of {len(schema_guids)})...")
                        
                        schema_response = self.get_schema_bulk(schema_batch)
                        
                        # Merge schema responses
                        schema_bulk_response["entities"].extend(schema_response.get("entities", []))
                        schema_bulk_response["referredEntities"].update(schema_response.get("referredEntities", {}))
                        
                        time.sleep(0.3)  # Rate limiting for sub-batches
                    
                    # Step 4: Extract metadata combining entity and schema data
                    print(f"  Step 4: Extracting metadata...")
                    batch_data = self.extract_metadata(entity_bulk_response, schema_bulk_response)
                    all_data.extend(batch_data)
                    
                    print(f"  ✓ Extracted {len(batch_data)} records from batch {batch_num}")
                else:
                    print(f"  No schema GUIDs found for this batch")
                    # Still extract table-level data
                    empty_schema_response = {"entities": [], "referredEntities": {}}
                    batch_data = self.extract_metadata(entity_bulk_response, empty_schema_response)
                    all_data.extend(batch_data)
                    print(f"  ✓ Extracted {len(batch_data)} table records (no columns)")
                
            except Exception as e:
                print(f"  ✗ Error processing batch {batch_num}: {str(e)}")
                continue
            
            time.sleep(0.5)  # Rate limiting between main batches
        
        return pd.DataFrame(all_data)
    
    def run_extraction(self) -> pd.DataFrame:
        """
        Main extraction workflow
        """
        # Authenticate
        self.get_access_token()
        
        # Search for entities of specified types
        all_entity_ids = []
        for entity_type in ENTITY_TYPES:
            entity_ids = self.search_entities(entity_type)
            print(f"Found {len(entity_ids)} entities of type {entity_type}")
            all_entity_ids.extend(entity_ids)
        
        print(f"\nTotal entities found: {len(all_entity_ids)}")
        
        if not all_entity_ids:
            print("No entities found. Exiting.")
            return pd.DataFrame()
        
        # Process entities in batches
        df = self.process_in_batches(all_entity_ids)
        
        print(f"\nExtraction complete. Total records: {len(df)}")
        return df


def upload_to_adls(df: pd.DataFrame, account_name: str, file_system: str, 
                   file_path: str, client_id: str, client_secret: str, tenant_id: str):
    """
    Upload DataFrame to Azure Data Lake Storage
    """
    try:
        # Create credentials
        authority = f"https://login.microsoftonline.com/{tenant_id}"
        scope = ["https://storage.azure.com/.default"]
        
        app = ConfidentialClientApplication(
            client_id=client_id,
            client_credential=client_secret,
            authority=authority
        )
        
        result = app.acquire_token_for_client(scopes=scope)
        
        if "access_token" not in result:
            raise Exception(f"ADLS Authentication failed: {result.get('error_description')}")
        
        token = result["access_token"]
        
        # Create DataLakeServiceClient
        service_client = DataLakeServiceClient(
            account_url=f"https://{account_name}.dfs.core.windows.net",
            credential=token
        )
        
        file_system_client = service_client.get_file_system_client(file_system=file_system)
        file_client = file_system_client.get_file_client(file_path)
        
        # Convert DataFrame to CSV
        csv_data = df.to_csv(index=False)
        
        # Upload file
        print(f"Uploading to ADLS: {account_name}/{file_system}/{file_path}")
        file_client.upload_data(csv_data, overwrite=True)
        
        print("Upload successful!")
        
    except Exception as e:
        print(f"Error uploading to ADLS: {str(e)}")
        raise


def main():
    """
    Main execution function
    """
    print("=" * 80)
    print("Azure Purview Data Extraction")
    print("=" * 80)
    print(f"Start Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
    
    # Initialize extractor
    extractor = AzurePurviewExtractor(
        account_name=ACCOUNT_NAME,
        tenant_id=TENANT_ID,
        client_id=CLIENT_ID,
        client_secret=CLIENT_SECRET
    )
    
    # Run extraction
    df = extractor.run_extraction()
    
    if df.empty:
        print("No data extracted. Exiting.")
        return
    
    # Display summary
    print("\n" + "=" * 80)
    print("DATA SUMMARY")
    print("=" * 80)
    print(f"Total Records: {len(df)}")
    print(f"Columns: {', '.join(df.columns)}")
    print(f"\nFirst 5 rows:")
    print(df.head())
    print(f"\nData types:")
    print(df.dtypes)
    
    # Save locally first
    local_file = f"purview_extract_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    df.to_csv(local_file, index=False)
    print(f"\nData saved locally: {local_file}")
    
    # Upload to ADLS
    print("\n" + "=" * 80)
    print("Uploading to Azure Data Lake Storage")
    print("=" * 80)
    
    upload_to_adls(
        df=df,
        account_name=ADLS_ACCOUNT_NAME,
        file_system=ADLS_FILE_SYSTEM,
        file_path=ADLS_PATH,
        client_id=CLIENT_ID,
        client_secret=CLIENT_SECRET,
        tenant_id=TENANT_ID
    )
    
    print("\n" + "=" * 80)
    print(f"End Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("Process completed successfully!")
    print("=" * 80)


if __name__ == "__main__":
    main()
